{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsaha205/Fall_22_PML/blob/main/PML_HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n"
      ],
      "metadata": {
        "id": "nMGT3_tN0gge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Framework:** Pytorch\n",
        "\n",
        "**Dataset:** FashionMNIST from torchvision.datasets (https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html).\n",
        "\n",
        "**Resources:**\n",
        "1. Optimizer: I used Adam optimizer from torch.optim (https://pytorch.org/docs/stable/optim.html).\n",
        "2. MLP: Used torch.nn for implementing 2-layer NN model (https://pytorch.org/docs/stable/nn.html).\n",
        "3. Plot: For plotting the data, I used matplotlib.pyplot library (https://matplotlib.org/3.5.1/api/_as_gen/matplotlib.pyplot.html).\n",
        "4. Loss function: For loss function, I used cross_entropy loss from F.cross_entropy (https://pytorch.org/docs/stable/nn.functional.html).\n",
        "5. Class Identify: To identify right class, I used log_softmax from F.log_softmax (https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html)."
      ],
      "metadata": {
        "id": "oybGznHCVvmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "m9L1n4M7YIZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "onHdRTJuYOOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as du\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "hueqaq_JN2hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read FashionMNIST data from torchvision.datasets\n",
        "train_data = datasets.FashionMNIST('.', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "test_data = datasets.FashionMNIST('.', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "# print the shape of the dataset\n",
        "print(\"train images:\", train_data.data.size())\n",
        "print(\"test images:\", test_data.data.size())"
      ],
      "metadata": {
        "id": "g3wzIkpTN6Pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1c05da-4683-4e88-fbb0-12470495435d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train images: torch.Size([60000, 28, 28])\n",
            "test images: torch.Size([10000, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the very first image of train dataset with label\n",
        "plt.imshow(train_data.data[0].numpy(), cmap='gray')\n",
        "plt.title('%i' % train_data.targets[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "mmr4TV2mPr1J",
        "outputId": "ff2ab126-9459-4534-ce9a-80c16e9b0f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the very first image of test dataset with label\n",
        "plt.imshow(test_data.data[0].numpy(), cmap='gray')\n",
        "plt.title('%i' % test_data.targets[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "OzN_kYs4GXqR",
        "outputId": "68e4704f-c284-4fa2-fd9b-8d10c7b6c95e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "KWWHQE6KZFaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        '''in_dim: input layer dim\n",
        "           hidden_dim: hidden layer dim\n",
        "           out_dim: output layer dim'''\n",
        "        \n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        # images are 28x28 so we need to flatten them into 784d vector\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        # #two fully connected layers\n",
        "        # self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        # self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "        \n",
        "        #three fully connected layers\n",
        "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 256)\n",
        "        self.fc3 = nn.Linear(256, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is 28x28, flatten it first\n",
        "        x = self.flatten(x)\n",
        "        \n",
        "        # compute output of fc1 and then apply relu activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # # for 2 layers\n",
        "        # # compute output layer, no activation needed, cross entropy will compute softmax\n",
        "        # x = self.fc2(x)\n",
        "\n",
        "        # for 3 layers\n",
        "        # compute output of fc2 and then apply relu activation\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # compute output layer, no activation needed, cross entropy will compute softmax\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "AbLxIfW8QFPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "3GdV0vH6O3w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "#hyper-paramets defining\n",
        "batch_size = 1000\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "in_dim = 28 # images are 28x28 as inputs\n",
        "hidden_dim = 512 # 512d hidden layer\n",
        "number_of_class = 10 # output is 10d since there are 10 classes\n",
        "\n",
        "# set model\n",
        "model = MLP(in_dim*in_dim, hidden_dim, number_of_class)\n",
        "\n",
        "# optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# load training data in batches\n",
        "train_loader = du.DataLoader(dataset=train_data,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=True)\n",
        "# send model over to device\n",
        "model = model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_ELw38ZY_6f",
        "outputId": "26d321a5-555e-42b6-b230-82b706d6e8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop over batches"
      ],
      "metadata": {
        "id": "nTbtmbCzPCYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):    \n",
        "    sum_valid_loss = 0.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # send batch over to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # zero out prev gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # run the forward pass\n",
        "        output = model(data)\n",
        "        \n",
        "        # compute loss/error\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # I used first 50000 datapoints of train_data as trainig dataset and last 10000 points as validation dataset\n",
        "        if batch_idx >= 50:\n",
        "          # sum up batch losses for validation dataset\n",
        "          sum_valid_loss += loss.item()\n",
        "          continue\n",
        "        \n",
        "        # compute gradients and take a step in training phase\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # average loss per example    \n",
        "    sum_valid_loss /= 10000\n",
        "    print(f'Epoch: {epoch}, Validation Loss: {sum_valid_loss:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYeYugjpZIgx",
        "outputId": "44972b4e-23eb-4d03-a93b-8a8b97f1a3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Validation Loss: 0.000454\n",
            "Epoch: 2, Validation Loss: 0.000375\n",
            "Epoch: 3, Validation Loss: 0.000356\n",
            "Epoch: 4, Validation Loss: 0.000319\n",
            "Epoch: 5, Validation Loss: 0.000308\n",
            "Epoch: 6, Validation Loss: 0.000298\n",
            "Epoch: 7, Validation Loss: 0.000307\n",
            "Epoch: 8, Validation Loss: 0.000307\n",
            "Epoch: 9, Validation Loss: 0.000270\n",
            "Epoch: 10, Validation Loss: 0.000264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "6MdlmpB5PQDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load test images in batches\n",
        "test_loader = du.DataLoader(dataset=test_data,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True)\n",
        "\n",
        "# set model in eval mode as we are no longer training\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# turning of gradient computation that will speed up testing\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        # send batches to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # compute forward pass and loss\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        \n",
        "        # sum up batch loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # get the class of the max log-probability\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        pred = output.max(dim=1)[1]\n",
        "\n",
        "        # add up number of correct predictions\n",
        "        correct += torch.sum(pred == target)\n",
        "  \n",
        "    # test loss per example\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    # final test accuracy\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    print(f'Test loss: {test_loss:.6f}, accuracy: {test_acc:.4f}', f'correct: {correct}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b54RWuKdZS58",
        "outputId": "fb72c271-3325-40ca-907d-c3a1947fd922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.000346, accuracy: 0.8784 correct: 8784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "B5m7jxs6FO6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters Used:\n",
        "1. batch_size - tried values (512, 1000, 1024)\n",
        "2. learning rate - tried values (0.1, 0.01, 0.001)\n",
        "3. epochs - tried values (5, 10)\n",
        "4. hidden_dim - tried values (256, 512)\n",
        "5. layer - tried values (2, 3, 4)\n",
        "\n",
        "Some best configs:\n",
        "1. learning rate (0.01), layer (2), epoch (5) - accuracy (0.8646)\n",
        "2. learning rate (0.001), layer (2), epoch (10) - accuracy (0.8625)\n",
        "3. learning rate (0.1), layer (2), epoch (10) - accuracy (0.8109)\n",
        "4. learning rate (0.01), layer (3), epoch (10) - accuracy (**0.8784**)\n",
        "\n",
        "I used different configurations of hyperparametes to get the best test accuracy. From the above some different configurations, we can see the test accuracy is the highest for learning rate 0.01 and layer 3. \n",
        "\n",
        "I did not use regularization, as I saw the dataset does not contain either of high bias or high variance rather the bias and variance are low. Also, the model is not actually overfitting for these small number of layers. Hence, use of regularization technic would not create big difference in this case (although the accuracy might improve in a bit). \n",
        "\n",
        "Yes, I used Adam optimization algorithm from torch.optim (https://pytorch.org/docs/stable/optim.html). Without optimizing, the model would not converge fast and hence it would perform very poor for this MLP network."
      ],
      "metadata": {
        "id": "nrXJRYx-FTCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4"
      ],
      "metadata": {
        "id": "aRL4-aSHPCfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Classifier Implementation"
      ],
      "metadata": {
        "id": "VKD1W8UYaiMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import tree\n",
        "\n",
        "# reshape 28x28 images to 784d vector\n",
        "train_data.data = torch.reshape(train_data.data, (60000, 784))\n",
        "test_data.data = torch.reshape(test_data.data, (10000, 784))\n",
        "\n",
        "criterion_list = ['gini', 'entropy']\n",
        "max_depth_list = [32, 64]\n",
        "min_samples_split_list = [16, 32]\n",
        "max_features_list = [64, 128, 256, 512]\n",
        "\n",
        "iter = 1\n",
        "optimum_criterion = None\n",
        "optimum_max_depth = None\n",
        "optimum_min_samples_split = None\n",
        "optimum_max_features = None\n",
        "max_accuracy_score = 0\n",
        "for criterion in criterion_list:\n",
        "  for max_depth in max_depth_list:\n",
        "    for min_samples_split in min_samples_split_list:\n",
        "      for max_features in max_features_list:\n",
        "        print('iter:', iter, ', criterion:', criterion, ', max_depth:', max_depth, ', min_samples_split:', min_samples_split, ', max_features:', max_features)\n",
        "        # Initialize our decision tree object\n",
        "        classification_tree = tree.DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, max_features=max_features)\n",
        "\n",
        "        # Train our decision tree (tree induction + pruning)\n",
        "        classification_tree = classification_tree.fit(train_data.data, train_data.targets)\n",
        "\n",
        "        # Predict\n",
        "        y_pred = classification_tree.predict(test_data.data, check_input=True)\n",
        "        accu_score = accuracy_score(y_pred, test_data.targets)\n",
        "        print(\"accu_score: %.4f\" % (accu_score))\n",
        "        \n",
        "        if accu_score > max_accuracy_score:\n",
        "          max_accuracy_score = accu_score\n",
        "          optimum_criterion = criterion\n",
        "          optimum_max_depth = max_depth\n",
        "          optimum_min_samples_split = min_samples_split\n",
        "          optimum_max_features = max_features\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "print('---------------Optimum results and configurations---------------')\n",
        "print(\"max_accuracy_score: %.4f\" % (max_accuracy_score))\n",
        "print('optimum_criterion:', optimum_criterion, ', optimum_max_depth:', optimum_max_depth, ', optimum_min_samples_split:', optimum_min_samples_split, ', optimum_max_features:', optimum_max_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRKzMXs5RBxH",
        "outputId": "6e958bce-2e6b-45e4-b326-0cdd871b59d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 1 , criterion: gini , max_depth: 32 , min_samples_split: 16 , max_features: 64\n",
            "accu_score: 0.7882\n",
            "iter: 2 , criterion: gini , max_depth: 32 , min_samples_split: 16 , max_features: 128\n",
            "accu_score: 0.7937\n",
            "iter: 3 , criterion: gini , max_depth: 32 , min_samples_split: 16 , max_features: 256\n",
            "accu_score: 0.7956\n",
            "iter: 4 , criterion: gini , max_depth: 32 , min_samples_split: 16 , max_features: 512\n",
            "accu_score: 0.7999\n",
            "iter: 5 , criterion: gini , max_depth: 32 , min_samples_split: 32 , max_features: 64\n",
            "accu_score: 0.7884\n",
            "iter: 6 , criterion: gini , max_depth: 32 , min_samples_split: 32 , max_features: 128\n",
            "accu_score: 0.7959\n",
            "iter: 7 , criterion: gini , max_depth: 32 , min_samples_split: 32 , max_features: 256\n",
            "accu_score: 0.7941\n",
            "iter: 8 , criterion: gini , max_depth: 32 , min_samples_split: 32 , max_features: 512\n",
            "accu_score: 0.7971\n",
            "iter: 9 , criterion: gini , max_depth: 64 , min_samples_split: 16 , max_features: 64\n",
            "accu_score: 0.7898\n",
            "iter: 10 , criterion: gini , max_depth: 64 , min_samples_split: 16 , max_features: 128\n",
            "accu_score: 0.7853\n",
            "iter: 11 , criterion: gini , max_depth: 64 , min_samples_split: 16 , max_features: 256\n",
            "accu_score: 0.7936\n",
            "iter: 12 , criterion: gini , max_depth: 64 , min_samples_split: 16 , max_features: 512\n",
            "accu_score: 0.7975\n",
            "iter: 13 , criterion: gini , max_depth: 64 , min_samples_split: 32 , max_features: 64\n",
            "accu_score: 0.7912\n",
            "iter: 14 , criterion: gini , max_depth: 64 , min_samples_split: 32 , max_features: 128\n",
            "accu_score: 0.7939\n",
            "iter: 15 , criterion: gini , max_depth: 64 , min_samples_split: 32 , max_features: 256\n",
            "accu_score: 0.7916\n",
            "iter: 16 , criterion: gini , max_depth: 64 , min_samples_split: 32 , max_features: 512\n",
            "accu_score: 0.7994\n",
            "iter: 17 , criterion: entropy , max_depth: 32 , min_samples_split: 16 , max_features: 64\n",
            "accu_score: 0.7918\n",
            "iter: 18 , criterion: entropy , max_depth: 32 , min_samples_split: 16 , max_features: 128\n",
            "accu_score: 0.7948\n",
            "iter: 19 , criterion: entropy , max_depth: 32 , min_samples_split: 16 , max_features: 256\n",
            "accu_score: 0.8029\n",
            "iter: 20 , criterion: entropy , max_depth: 32 , min_samples_split: 16 , max_features: 512\n",
            "accu_score: 0.7997\n",
            "iter: 21 , criterion: entropy , max_depth: 32 , min_samples_split: 32 , max_features: 64\n",
            "accu_score: 0.8008\n",
            "iter: 22 , criterion: entropy , max_depth: 32 , min_samples_split: 32 , max_features: 128\n",
            "accu_score: 0.8034\n",
            "iter: 23 , criterion: entropy , max_depth: 32 , min_samples_split: 32 , max_features: 256\n",
            "accu_score: 0.8027\n",
            "iter: 24 , criterion: entropy , max_depth: 32 , min_samples_split: 32 , max_features: 512\n",
            "accu_score: 0.8028\n",
            "iter: 25 , criterion: entropy , max_depth: 64 , min_samples_split: 16 , max_features: 64\n",
            "accu_score: 0.7942\n",
            "iter: 26 , criterion: entropy , max_depth: 64 , min_samples_split: 16 , max_features: 128\n",
            "accu_score: 0.7970\n",
            "iter: 27 , criterion: entropy , max_depth: 64 , min_samples_split: 16 , max_features: 256\n",
            "accu_score: 0.7972\n",
            "iter: 28 , criterion: entropy , max_depth: 64 , min_samples_split: 16 , max_features: 512\n",
            "accu_score: 0.8041\n",
            "iter: 29 , criterion: entropy , max_depth: 64 , min_samples_split: 32 , max_features: 64\n",
            "accu_score: 0.7980\n",
            "iter: 30 , criterion: entropy , max_depth: 64 , min_samples_split: 32 , max_features: 128\n",
            "accu_score: 0.8027\n",
            "iter: 31 , criterion: entropy , max_depth: 64 , min_samples_split: 32 , max_features: 256\n",
            "accu_score: 0.8061\n",
            "iter: 32 , criterion: entropy , max_depth: 64 , min_samples_split: 32 , max_features: 512\n",
            "accu_score: 0.8100\n",
            "---------------Optimum results and configurations---------------\n",
            "max_accuracy_score: 0.8100\n",
            "optimum_criterion: entropy , optimum_max_depth: 64 , optimum_min_samples_split: 32 , optimum_max_features: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried Decision Tree Classifier as the other baseline method from scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and tried different configurations of hyper-parameter, printed with test accuracy above. I got the best test accuracy of 0.8100 for the criterion: entropy, max_depth: 64, min_samples_split: 32 and max_features: 512. \n",
        "\n",
        "\n",
        "Best test accuracy for MLP: 0.8784\n",
        "\n",
        "Best test accuracy for Decision Tree: 0.8100\n",
        "\n",
        "From the above results, we can see 3-layer MLP works better than Decision Tree for this FashionMNIST dataset. The reason is that FashionMNIST is a large dataset with 60000 datapoints, 784 (28x28) features and 10 classes. Being a baseline model, Decision Tree works well for the small number of datasets with small features and classes. Here MLP wins over Decision Tree classifier."
      ],
      "metadata": {
        "id": "q3inVMAHak85"
      }
    }
  ]
}