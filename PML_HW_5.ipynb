{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4de049618a674f12b251b72656135088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76a17f96d0c5412f976e1ae1f7ef5201",
              "IPY_MODEL_24f881ca714843a1a15f7d9a6194d2ea",
              "IPY_MODEL_ceed1efee8a3497a88513939007d6e0e"
            ],
            "layout": "IPY_MODEL_9d49988812574a67ad39f62f495166e5"
          }
        },
        "76a17f96d0c5412f976e1ae1f7ef5201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdfe2d9a4de64ec5a3a70065c791b8ce",
            "placeholder": "​",
            "style": "IPY_MODEL_cfed90bc1a8342f489b6eb26c01373dc",
            "value": "100%"
          }
        },
        "24f881ca714843a1a15f7d9a6194d2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29ac02f2d173405d84345613b6ad8777",
            "max": 52147035,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9de90abb7885449a8ca8d9303f26215b",
            "value": 52147035
          }
        },
        "ceed1efee8a3497a88513939007d6e0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08147caf8d2c4f3cb2202cfc62556253",
            "placeholder": "​",
            "style": "IPY_MODEL_8f6a84bace1a4f7bacbc9e95efc15205",
            "value": " 49.7M/49.7M [00:01&lt;00:00, 19.6MB/s]"
          }
        },
        "9d49988812574a67ad39f62f495166e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdfe2d9a4de64ec5a3a70065c791b8ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfed90bc1a8342f489b6eb26c01373dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29ac02f2d173405d84345613b6ad8777": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9de90abb7885449a8ca8d9303f26215b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08147caf8d2c4f3cb2202cfc62556253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f6a84bace1a4f7bacbc9e95efc15205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsaha205/Fall_22_PML/blob/main/PML_HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n"
      ],
      "metadata": {
        "id": "nMGT3_tN0gge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1"
      ],
      "metadata": {
        "id": "a0AKAyxdpbMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Framework:** Pytorch\n",
        "\n",
        "**Dataset:** FashionMNIST from torchvision.datasets (https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html).\n",
        "\n",
        "**Resources:**\n",
        "1. Optimizer: I used Adam optimizer from torch.optim (https://pytorch.org/docs/stable/optim.html).\n",
        "2. CNN: Used torch.nn for implementing 2-layer CNN model (https://pytorch.org/docs/stable/nn.html).\n",
        "3. Plot: For plotting the data, I used matplotlib.pyplot library (https://matplotlib.org/3.5.1/api/_as_gen/matplotlib.pyplot.html).\n",
        "4. Loss function: For loss function, I used cross_entropy loss from F.cross_entropy (https://pytorch.org/docs/stable/nn.functional.html).\n",
        "5. Class Identify: To identify right class, I used log_softmax from F.log_softmax (https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html)."
      ],
      "metadata": {
        "id": "oybGznHCVvmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "onHdRTJuYOOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as du\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "hueqaq_JN2hy"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read FashionMNIST data from torchvision.datasets\n",
        "train_data = datasets.FashionMNIST('.', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "test_data = datasets.FashionMNIST('.', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "# print the shape of the dataset\n",
        "print(\"train images:\", train_data.data.size())\n",
        "print(\"test images:\", test_data.data.size())"
      ],
      "metadata": {
        "id": "g3wzIkpTN6Pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f47ce71-eb20-4fad-e644-c4df28ccefe5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train images: torch.Size([60000, 28, 28])\n",
            "test images: torch.Size([10000, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the very first image of train dataset with label\n",
        "plt.imshow(train_data.data[0].numpy(), cmap='gray')\n",
        "plt.title('%i' % train_data.targets[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "mmr4TV2mPr1J",
        "outputId": "7ac96dbb-a48f-45c4-9d32-71e7b8ffcf74"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the very first image of test dataset with label\n",
        "plt.imshow(test_data.data[0].numpy(), cmap='gray')\n",
        "plt.title('%i' % test_data.targets[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "OzN_kYs4GXqR",
        "outputId": "fd663401-76fd-4582-c731-1c9ffa50d7dd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model"
      ],
      "metadata": {
        "id": "KWWHQE6KZFaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, k_size, k_size_max_pooling, dropout_prob):\n",
        "        '''in_dim: input layer dim\n",
        "           hidden_dim: hidden layer dim\n",
        "           out_dim: output layer dim'''\n",
        "        \n",
        "        super(CNN, self).__init__() \n",
        "             \n",
        "        self.cnn = nn.Conv1d(in_dim, in_dim, k_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.maxpool = nn.MaxPool1d(k_size_max_pooling)\n",
        "        \n",
        "        # images are 28x28 so flatten them into 784d vec\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        #two fully connected layers\n",
        "        self.fc1 = nn.Linear(140, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = self.cnn(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        x = F.relu(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        x = self.maxpool(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        # since x is 28x28, flatten it first\n",
        "        x = self.flatten(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        # compute output of fc1, and apply relu activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # print(x.shape)\n",
        "\n",
        "        # compute output layer, no activation: cross entropy will compute softmax\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AbLxIfW8QFPo"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "3GdV0vH6O3w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "#hyper-paramets defining\n",
        "batch_size = 1000\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "in_dim = 28 # images are 28x28 as inputs\n",
        "hidden_dim = 512 # 512d hidden layer\n",
        "number_of_class = 10 # output is 10d since there are 10 classes\n",
        "k_size = 9 # kernel size of CNN\n",
        "k_size_max_pooling = 4 # kernel size of max pooling\n",
        "dropout_prob = 0.2 # drop out probability\n",
        "\n",
        "# set model\n",
        "model = CNN(in_dim, hidden_dim, number_of_class, k_size, k_size_max_pooling, dropout_prob)\n",
        "\n",
        "# optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# load training data in batches\n",
        "train_loader = du.DataLoader(dataset=train_data,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=True)\n",
        "# send model over to device\n",
        "model = model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_ELw38ZY_6f",
        "outputId": "32a41253-eb67-459e-b632-998827711625"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (cnn): Conv1d(28, 28, kernel_size=(9,), stride=(1,))\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (maxpool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=140, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop over batches"
      ],
      "metadata": {
        "id": "nTbtmbCzPCYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):    \n",
        "    sum_valid_loss = 0.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        data = torch.squeeze(data) / 255\n",
        "        # send batch over to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # zero out prev gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # run the forward pass\n",
        "        output = model(data)\n",
        "        \n",
        "        # compute loss/error\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # I used first 50000 datapoints of train_data as trainig dataset and last 10000 points as validation dataset\n",
        "        if batch_idx >= 50:\n",
        "          # sum up batch losses for validation dataset\n",
        "          sum_valid_loss += loss.item()\n",
        "          continue\n",
        "        \n",
        "        # compute gradients and take a step in training phase\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # average loss per example    \n",
        "    sum_valid_loss /= 10000\n",
        "    print(f'Epoch: {epoch}, Validation Loss: {sum_valid_loss:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYeYugjpZIgx",
        "outputId": "d7e7abe9-048c-4915-a31d-edf08df1d510"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Validation Loss: 0.000961\n",
            "Epoch: 2, Validation Loss: 0.000759\n",
            "Epoch: 3, Validation Loss: 0.000680\n",
            "Epoch: 4, Validation Loss: 0.000608\n",
            "Epoch: 5, Validation Loss: 0.000579\n",
            "Epoch: 6, Validation Loss: 0.000564\n",
            "Epoch: 7, Validation Loss: 0.000556\n",
            "Epoch: 8, Validation Loss: 0.000541\n",
            "Epoch: 9, Validation Loss: 0.000498\n",
            "Epoch: 10, Validation Loss: 0.000482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "6MdlmpB5PQDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load test images in batches\n",
        "test_loader = du.DataLoader(dataset=test_data,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True)\n",
        "\n",
        "# set model in eval mode as we are no longer training\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# turning of gradient computation that will speed up testing\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        \n",
        "        data = torch.squeeze(data) / 255\n",
        "        # send batches to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # compute forward pass and loss\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        \n",
        "        # sum up batch loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # get the class of the max log-probability\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        pred = output.max(dim=1)[1]\n",
        "\n",
        "        # add up number of correct predictions\n",
        "        correct += torch.sum(pred == target)\n",
        "  \n",
        "    # test loss per example\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    # final test accuracy\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    print(f'Test loss: {test_loss:.6f}, accuracy: {test_acc:.4f}', f'correct: {correct}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b54RWuKdZS58",
        "outputId": "fddd5bb5-4e49-41a9-b5b0-3e1b5aa8aec5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.000520, accuracy: 0.8207 correct: 8207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters Used:\n",
        "1. batch_size - tried values (512, 1000, 1024)\n",
        "2. learning rate - tried values (0.1, 0.01, 0.001)\n",
        "3. epochs - tried values (5, 10)\n",
        "4. hidden_dim - tried values (256, 512)\n",
        "5. layer - tried values (2, 3, 4)\n",
        "\n",
        "Some best configs:\n",
        "1. learning rate (0.01), layer (2), epoch (5) - accuracy (0.8646)\n",
        "2. learning rate (0.001), layer (2), epoch (10) - accuracy (0.8625)\n",
        "3. learning rate (0.1), layer (2), epoch (10) - accuracy (0.8109)\n",
        "4. learning rate (0.01), layer (3), epoch (10) - accuracy (**0.8784**)\n",
        "\n",
        "I used different configurations of hyperparametes to get the best test accuracy. From the above some different configurations, we can see the test accuracy is the highest for learning rate 0.01 and layer 3."
      ],
      "metadata": {
        "id": "nrXJRYx-FTCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2"
      ],
      "metadata": {
        "id": "aRL4-aSHPCfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform =   transforms.Compose([\n",
        "#         transforms.Resize((224, 224)),\n",
        "#         transforms.RandomHorizontalFlip(),\n",
        "#         transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
        "#         transforms.RandomAffine(degrees=40, translate=None, scale=(1, 2), shear=15, resample=False, fillcolor=0),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "# ])\n",
        "\n",
        "# model_ft = models.vgg16(pretrained=True)\n",
        "# num_ftrs = model_ft.classifier[0].in_features\n",
        "# model_ft.classifier[0] = nn.Linear(num_ftrs, number_of_class)\n",
        "\n",
        "\n",
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4de049618a674f12b251b72656135088",
            "76a17f96d0c5412f976e1ae1f7ef5201",
            "24f881ca714843a1a15f7d9a6194d2ea",
            "ceed1efee8a3497a88513939007d6e0e",
            "9d49988812574a67ad39f62f495166e5",
            "cdfe2d9a4de64ec5a3a70065c791b8ce",
            "cfed90bc1a8342f489b6eb26c01373dc",
            "29ac02f2d173405d84345613b6ad8777",
            "9de90abb7885449a8ca8d9303f26215b",
            "08147caf8d2c4f3cb2202cfc62556253",
            "8f6a84bace1a4f7bacbc9e95efc15205"
          ]
        },
        "id": "SoM9IJ3RwiSz",
        "outputId": "db1f0046-85e7-4825-dd51-be157e2812a8"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/49.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4de049618a674f12b251b72656135088"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogLeNet(\n",
              "  (conv1): BasicConv2d(\n",
              "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (conv2): BasicConv2d(\n",
              "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv3): BasicConv2d(\n",
              "    (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (inception3a): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception3b): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (inception4a): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception4b): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception4c): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception4d): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception4e): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (inception5a): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (inception5b): Inception(\n",
              "    (branch1): BasicConv2d(\n",
              "      (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch2): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): BasicConv2d(\n",
              "        (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
              "      (1): BasicConv2d(\n",
              "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (aux1): None\n",
              "  (aux2): None\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "# input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(train_data)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
        "print(output[0])\n",
        "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "print(probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "AYDVLe6ISApw",
        "outputId": "abc2e3ee-b4f7-4e15-bfb9-30c9a4581208"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-a0f891d20a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m ])\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0minput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create a mini-batch as expected by the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be PIL Image. Got {type(img)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'torchvision.datasets.mnist.FashionMNIST'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer (Adam)\n",
        "optimizer = optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
        "\n",
        "# send model over to device\n",
        "model_ft = model_ft.to(device)\n",
        "model_ft.train()\n",
        "\n",
        "for epoch in range(1, epochs + 1):    \n",
        "    sum_valid_loss = 0.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        data = torch.squeeze(data)\n",
        "        # send batch over to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # zero out prev gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # run the forward pass\n",
        "        output = model_ft(data)\n",
        "        \n",
        "        # compute loss/error\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # I used first 50000 datapoints of train_data as trainig dataset and last 10000 points as validation dataset\n",
        "        if batch_idx >= 50:\n",
        "          # sum up batch losses for validation dataset\n",
        "          sum_valid_loss += loss.item()\n",
        "          continue\n",
        "        \n",
        "        # compute gradients and take a step in training phase\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # average loss per example    \n",
        "    sum_valid_loss /= 10000\n",
        "    print(f'Epoch: {epoch}, Validation Loss: {sum_valid_loss:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "0id-bu8iOqX7",
        "outputId": "fcfebbe8-f59e-4f90-c5bb-8c108a3dc65b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-ea9a010cfaf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# run the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# compute loss/error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[1, 1000, 28, 28] to have 3 channels, but got 1000 channels instead"
          ]
        }
      ]
    }
  ]
}