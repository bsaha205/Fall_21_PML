{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsaha205/Fall_22_PML/blob/main/PML_HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n"
      ],
      "metadata": {
        "id": "nMGT3_tN0gge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1"
      ],
      "metadata": {
        "id": "a0AKAyxdpbMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Framework:** Pytorch\n",
        "\n",
        "**Dataset:** FashionMNIST from torchvision.datasets (https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html).\n",
        "\n",
        "**Resources:**\n",
        "1. Optimizer: I used Adam optimizer from torch.optim (https://pytorch.org/docs/stable/optim.html).\n",
        "2. CNN: Used torch.nn for implementing 2-layer CNN model (https://pytorch.org/docs/stable/nn.html).\n",
        "3. Plot: For plotting the data, I used matplotlib.pyplot library (https://matplotlib.org/3.5.1/api/_as_gen/matplotlib.pyplot.html).\n",
        "4. Loss function: For loss function, I used cross_entropy loss from F.cross_entropy (https://pytorch.org/docs/stable/nn.functional.html).\n",
        "5. Class Identify: To identify right class, I used log_softmax from F.log_softmax (https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html)."
      ],
      "metadata": {
        "id": "oybGznHCVvmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "onHdRTJuYOOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as du\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "hueqaq_JN2hy"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read FashionMNIST data from torchvision.datasets\n",
        "train_data = datasets.FashionMNIST('.', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "test_data = datasets.FashionMNIST('.', train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "# print the shape of the dataset\n",
        "print(\"train images:\", train_data.data.size())\n",
        "print(\"test images:\", test_data.data.size())"
      ],
      "metadata": {
        "id": "g3wzIkpTN6Pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a366e9f-0a89-45bd-b422-53e6b965aa3b"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train images: torch.Size([60000, 28, 28])\n",
            "test images: torch.Size([10000, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the very first image of train dataset with label\n",
        "plt.imshow(train_data.data[0].numpy(), cmap='gray')\n",
        "plt.title('%i' % train_data.targets[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "mmr4TV2mPr1J",
        "outputId": "bce5eb0b-88c1-475f-faf2-ec8512b6bdac"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the very first image of test dataset with label\n",
        "plt.imshow(test_data.data[0].numpy(), cmap='gray')\n",
        "plt.title('%i' % test_data.targets[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "OzN_kYs4GXqR",
        "outputId": "20133a57-67be-4d69-f09b-9d018aa55e58"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQLUlEQVR4nO3df4xVZX7H8c9HRFQERRAEVsWuiC66ug0RFW1t/IH1H39Fs/zRUEvCmqxNN6lNzTbNmjRN1qa7Te0fm7LRyLZbt5sokZjSldqmbtNURUIRfy24gcgEmCIoiKAC3/5xD82sznme8Z5751z7vF/JZO7c75y5X+7Mh3Pufc5zHkeEAPz/d1LbDQAYH4QdKARhBwpB2IFCEHagEIQdKARhBwpB2DEq25fa/lfb79veZvvOtntCM4Qdn2H7ZEnPSHpW0tmSVkr6e9sXt9oYGjFn0OHTbF8m6b8kTYnqD8T2c5JejIg/bbU5dI09O8bKki5ruwl0j7BjNG9JGpb0R7Yn2r5F0m9KOr3dttAEh/EYle2vSvobdfbmGyT9j6SPImJFq42ha4QdY2L7PyWtjoi/bbsXdIfDeIzK9ldtn2r7dNsPSpot6YmW20IDhB11fkfSLnVeu98o6eaI+KjdltAEh/FAIdizA4Ug7EAhCDtQCMIOFOLk8Xww27wbCPRZRHi0+xvt2W3favutagrkQ01+FoD+6nrozfYESb+QdLOknZJelrQsIl5PbMOeHeizfuzZr5K0LSJ+GREfS/qJpNsb/DwAfdQk7HMlvTPi653Vfb/C9krbG2xvaPBYABrq+xt0EbFK0iqJw3igTU327EOSzhvx9Zeq+wAMoCZhf1nSfNsX2j5F0tclre1NWwB6revD+Ig4avsBST+TNEHS4xHxWs86A9BT4zrrjdfsQP/15aQaAF8chB0oBGEHCkHYgUIQdqAQhB0oBGEHCkHYgUIQdqAQhB0oBGEHCkHYgUIQdqAQhB0oBGEHCkHYgUIQdqAQhB0oBGEHCkHYgUIQdqAQhB0oBGEHCkHYgUIQdqAQhB0oBGEHCkHYgUIQdqAQhB0oRNfrs0uS7e2SDko6JuloRCzqRVMAeq9R2Cu/FRF7e/BzAPQRh/FAIZqGPSQ9Z/sV2ytH+wbbK21vsL2h4WMBaMAR0f3G9tyIGLI9U9J6Sb8fES8kvr/7BwMwJhHh0e5vtGePiKHq87CkNZKuavLzAPRP12G3Pdn2lBO3Jd0iaUuvGgPQW03ejZ8laY3tEz/nHyLin3vSFYCea/Sa/XM/GK/Zgb7ry2t2AF8chB0oBGEHCkHYgUIQdqAQvZgIA7RiwoQJyfrx48dra01HoSZNmpSsf/TRR8n6RRddVFvbtm1bVz3lsGcHCkHYgUIQdqAQhB0oBGEHCkHYgUIQdqAQjLMXrpqi3HU9NZYtSXPnzq2tXXPNNclt161bl6wfOnQoWe+n3Dh6zt13311be+SRRxr97Drs2YFCEHagEIQdKARhBwpB2IFCEHagEIQdKATj7EjKjaPnXH/99bW1xYsXJ7edM2dOsv7oo4921VMvzJw5M1lfunRpsn7gwIFetjMm7NmBQhB2oBCEHSgEYQcKQdiBQhB2oBCEHSgE4+yFy117/ejRo8n6okWLkvVLL720trZnz57ktvPnz0/W16xZk6zv27evtnbaaaclt92xY0eyPn369GR96tSpyfrOnTuT9X7I7tltP2572PaWEfedbXu97a3V52n9bRNAU2M5jH9C0q2fuu8hSc9HxHxJz1dfAxhg2bBHxAuSPn08dLuk1dXt1ZLu6HFfAHqs29fssyJiV3V7t6RZdd9oe6WklV0+DoAeafwGXUSE7dpV8iJilaRVkpT6PgD91e3Q2x7bsyWp+jzcu5YA9EO3YV8raXl1e7mkZ3rTDoB+yR7G235S0g2SZtjeKek7kr4r6ae2V0jaIenefjaJ7p10Uvr/89w4+uTJk5P1e+65J1lPXV/91FNPTW47ZcqUZD13TfvUvz237cKFC5P1d955J1nfv39/sn7yyeN/ikv2ESNiWU3pxh73AqCPOF0WKARhBwpB2IFCEHagEIQdKARTXMcoNVQTkT4xMDf8lds+V09NUz127Fhy25z7778/Wd+9e3eyfuTIkdravHnzktvmhuZyU2RTz0vuEtm55aA//vjjZD03xXXSpEm1tdxwZ7dLVbNnBwpB2IFCEHagEIQdKARhBwpB2IFCEHagEMWMs+emNDYd605puuxx7nLPTcbSly2rm9TYce655ybrGzduTNYnTpxYWzvrrLOS27777rvJeupS0ZI0Y8aM2lpu+mzuOc/JnVtx+umn19Zyl9DetGlTdz11tRWALxzCDhSCsAOFIOxAIQg7UAjCDhSCsAOFKGacvck4uZQeN82NqebGwXO9NRlHv++++5L1BQsWJOu5SyanxrKl9PkNuWWTh4aGkvXcWHnq/IYPP/wwuW1uLn3T8zZSli5dmqwzzg4gibADhSDsQCEIO1AIwg4UgrADhSDsQCG+UOPsufHslNy4Z27cNDVm23S+es6cOXOS9bvuuqu2lhvL3rp1a7J+xhlnJOup659L0vTp02truWuv535nqTnhOblzF1JLTY9l+9y13VN/M0uWLElu261semw/bnvY9pYR9z1se8j2purjtr50B6BnxrKrfELSraPc/1cRcWX18U+9bQtAr2XDHhEvSEpf/wfAwGvyBt0DtjdXh/nT6r7J9krbG2xvaPBYABrqNuw/kPRlSVdK2iXpe3XfGBGrImJRRCzq8rEA9EBXYY+IPRFxLCKOS/qhpKt62xaAXusq7LZnj/jyTklb6r4XwGDIjrPbflLSDZJm2N4p6TuSbrB9paSQtF3SN8b6gE3WEu/neHaT+cfnnHNOsn7BBRck65dcckmyPnv27GQ9NV594MCB5La5a7fn1hlPXRdeSo/D536fuect99jvvfdebe2TTz5JbpvrLXfOx+HDh5P1VA4OHjyY3HbhwoW1tbfffru2lg17RIy2isBjue0ADBZOlwUKQdiBQhB2oBCEHSgEYQcKMe5TXJtcFnnWrFm1tdwwzeTJkxvVU1NFL7zwwuS2uamYuWGgDz74IFlPDQOdeeaZyW1zU2CPHj2arOf+balLNuemkZ5yyinJ+q5du5L11L891/f+/fuT9dzU32nTas8gl5SeAptbJjs1bXjHjh21NfbsQCEIO1AIwg4UgrADhSDsQCEIO1AIwg4UYqAuJX3TTTcl66lLKufGqmfOnJms56YspqY85h47N2UxN2abG3dNXQY7d6nn3Hhy7nnJ9Z6aypm73HLueXv//feT9dzvvInc85abIps6vyF3fkHq3IfUVG327EAhCDtQCMIOFIKwA4Ug7EAhCDtQCMIOFGJcx9mnTp2qq6++ura+YsWK5PZvvvlmbS03tzl3SeXUeLCUvlxzbtuc3Hhybtw1dY2A3KWgc0tV5+a758aTU5d7zp0/kLp+gZS+pHLusZv+znLnCOTmyx85cqTrnz08PFxbS43Bs2cHCkHYgUIQdqAQhB0oBGEHCkHYgUIQdqAQY1my+TxJP5I0S50lmldFxF/bPlvSP0qap86yzfdGRHKS76FDh/TSSy/V1lNj8JJ0+eWX19aWLFmS3DYnd3301Fj4vn37ktvm6rl52blx9tRYeeoa45K0YMGCZD03Xpwbx0/Nr77iiiuS227evDlZ3759e7Keuj5Cbp5/kyW8pfzf09DQUG0td05I6hoCqesPjGXPflTSH0bEVyRdLembtr8i6SFJz0fEfEnPV18DGFDZsEfErojYWN0+KOkNSXMl3S5pdfVtqyXd0a8mATT3uV6z254n6WuSXpQ0KyJOnKO6W53DfAADasznxts+Q9JTkr4VEQdGvk6MiLA96osc2yslraxuN+sWQNfGtGe3PVGdoP84Ip6u7t5je3ZVny1p1LPzI2JVRCyKiEW5ixcC6J9s+tzZHT8m6Y2I+P6I0lpJy6vbyyU90/v2APSKc0MMtq+T9HNJr0o6MZ/x2+q8bv+ppPMl7VBn6C05xlR3qN8LuUsaL168OFm/+OKLk/Vrr722tpa7ZHFueCq3XHTu5U/qd5ibgpobFkxNK5ak9evXJ+vr1q2rraWmefbC2rVra2vnn39+ctu9e/cm67lpybl6amgut5T1gw8+WFs7fPiwjh07NuofTPY1e0T8h6S6v7Ybc9sDGAy8iAYKQdiBQhB2oBCEHSgEYQcKQdiBQmTH2Xv6YH0cZwfQERGjDpWzZwcKQdiBQhB2oBCEHSgEYQcKQdiBQhB2oBCEHSgEYQcKQdiBQhB2oBCEHSgEYQcKQdiBQhB2oBCEHSgEYQcKQdiBQhB2oBCEHSgEYQcKQdiBQhB2oBDZsNs+z/a/2X7d9mu2/6C6/2HbQ7Y3VR+39b9dAN3KLhJhe7ak2RGx0fYUSa9IukPSvZI+iIi/HPODsUgE0Hd1i0ScPIYNd0naVd0+aPsNSXN72x6Afvtcr9ltz5P0NUkvVnc9YHuz7cdtT6vZZqXtDbY3NOoUQCNjXuvN9hmS/l3Sn0fE07ZnSdorKST9mTqH+r+X+RkcxgN9VncYP6aw254o6VlJP4uI749Snyfp2Yi4LPNzCDvQZ10v7Gjbkh6T9MbIoFdv3J1wp6QtTZsE0D9jeTf+Okk/l/SqpOPV3d+WtEzSleocxm+X9I3qzbzUz2LPDvRZo8P4XiHsQP+xPjtQOMIOFIKwA4Ug7EAhCDtQCMIOFIKwA4Ug7EAhCDtQCMIOFIKwA4Ug7EAhCDtQCMIOFCJ7wcke2ytpx4ivZ1T3DaJB7W1Q+5LorVu97O2CusK4zmf/zIPbGyJiUWsNJAxqb4Pal0Rv3Rqv3jiMBwpB2IFCtB32VS0/fsqg9jaofUn01q1x6a3V1+wAxk/be3YA44SwA4VoJey2b7X9lu1tth9qo4c6trfbfrVahrrV9emqNfSGbW8Zcd/Zttfb3lp9HnWNvZZ6G4hlvBPLjLf63LW9/Pm4v2a3PUHSLyTdLGmnpJclLYuI18e1kRq2t0taFBGtn4Bh+zckfSDpRyeW1rL9F5L2RcR3q/8op0XEHw9Ibw/rcy7j3afe6pYZ/121+Nz1cvnzbrSxZ79K0raI+GVEfCzpJ5Jub6GPgRcRL0ja96m7b5e0urq9Wp0/lnFX09tAiIhdEbGxun1Q0ollxlt97hJ9jYs2wj5X0jsjvt6pwVrvPSQ9Z/sV2yvbbmYUs0Yss7Vb0qw2mxlFdhnv8fSpZcYH5rnrZvnzpniD7rOui4hfl/Tbkr5ZHa4OpOi8BhuksdMfSPqyOmsA7pL0vTabqZYZf0rStyLiwMham8/dKH2Ny/PWRtiHJJ034usvVfcNhIgYqj4PS1qjzsuOQbLnxAq61efhlvv5PxGxJyKORcRxST9Ui89dtcz4U5J+HBFPV3e3/tyN1td4PW9thP1lSfNtX2j7FElfl7S2hT4+w/bk6o0T2Z4s6RYN3lLUayUtr24vl/RMi738ikFZxrtumXG1/Ny1vvx5RIz7h6Tb1HlH/m1Jf9JGDzV9/Zqk/64+Xmu7N0lPqnNY94k6722skDRd0vOStkr6F0lnD1Bvf6fO0t6b1QnW7JZ6u06dQ/TNkjZVH7e1/dwl+hqX543TZYFC8AYdUAjCDhSCsAOFIOxAIQg7UAjCDhSCsAOF+F/QiDYLHl4y2AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model"
      ],
      "metadata": {
        "id": "KWWHQE6KZFaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, k_size, k_size_max_pooling, dropout_prob):\n",
        "        '''in_dim: input layer dim\n",
        "           hidden_dim: hidden layer dim\n",
        "           out_dim: output layer dim'''\n",
        "        \n",
        "        super(CNN, self).__init__() \n",
        "             \n",
        "        self.cnn = nn.Conv1d(in_dim, in_dim, k_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.maxpool = nn.MaxPool1d(k_size_max_pooling)\n",
        "        \n",
        "        # images are 28x28 so flatten them into 784d vec\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        #two fully connected layers\n",
        "        self.fc1 = nn.Linear(140, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = self.cnn(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        x = F.relu(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        x = self.maxpool(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        # since x is 28x28, flatten it first\n",
        "        x = self.flatten(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        # compute output of fc1, and apply relu activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # print(x.shape)\n",
        "\n",
        "        # compute output layer, no activation: cross entropy will compute softmax\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AbLxIfW8QFPo"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "3GdV0vH6O3w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "#hyper-paramets defining\n",
        "batch_size = 1000\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "in_dim = 28 # images are 28x28 as inputs\n",
        "hidden_dim = 512 # 512d hidden layer\n",
        "number_of_class = 10 # output is 10d since there are 10 classes\n",
        "k_size = 9 # kernel size of CNN\n",
        "k_size_max_pooling = 4 # kernel size of max pooling\n",
        "dropout_prob = 0.2 # drop out probability\n",
        "\n",
        "# set model\n",
        "model = CNN(in_dim*in_dim, hidden_dim, number_of_class, k_size, k_size_max_pooling, dropout_prob)\n",
        "\n",
        "# optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# load training data in batches\n",
        "train_loader = du.DataLoader(dataset=train_data,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=True)\n",
        "# send model over to device\n",
        "model = model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_ELw38ZY_6f",
        "outputId": "7a209847-a95e-4d8f-bb1d-079a6fe2e614"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (cnn): Conv1d(784, 784, kernel_size=(9,), stride=(1,))\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (maxpool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=140, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop over batches"
      ],
      "metadata": {
        "id": "nTbtmbCzPCYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):    \n",
        "    sum_valid_loss = 0.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        data = torch.squeeze(data)\n",
        "        # send batch over to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # zero out prev gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # run the forward pass\n",
        "        output = model(data)\n",
        "        \n",
        "        # compute loss/error\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # I used first 50000 datapoints of train_data as trainig dataset and last 10000 points as validation dataset\n",
        "        if batch_idx >= 50:\n",
        "          # sum up batch losses for validation dataset\n",
        "          sum_valid_loss += loss.item()\n",
        "          continue\n",
        "        \n",
        "        # compute gradients and take a step in training phase\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # average loss per example    \n",
        "    sum_valid_loss /= 10000\n",
        "    print(f'Epoch: {epoch}, Validation Loss: {sum_valid_loss:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "eYeYugjpZIgx",
        "outputId": "43bdc343-9025-4741-d5cc-27860c743f31"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-fcef0fe3ec94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# run the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# compute loss/error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-151-0540a11583da>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    303\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 304\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [784, 784, 9], expected input[1000, 28, 28] to have 784 channels, but got 28 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "6MdlmpB5PQDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load test images in batches\n",
        "test_loader = du.DataLoader(dataset=test_data,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True)\n",
        "\n",
        "# set model in eval mode as we are no longer training\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# turning of gradient computation that will speed up testing\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        data = torch.squeeze(data)\n",
        "        # send batches to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # compute forward pass and loss\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        \n",
        "        # sum up batch loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # get the class of the max log-probability\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        pred = output.max(dim=1)[1]\n",
        "\n",
        "        # add up number of correct predictions\n",
        "        correct += torch.sum(pred == target)\n",
        "  \n",
        "    # test loss per example\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    # final test accuracy\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    print(f'Test loss: {test_loss:.6f}, accuracy: {test_acc:.4f}', f'correct: {correct}')"
      ],
      "metadata": {
        "id": "b54RWuKdZS58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters Used:\n",
        "1. batch_size - tried values (512, 1000, 1024)\n",
        "2. learning rate - tried values (0.1, 0.01, 0.001)\n",
        "3. epochs - tried values (5, 10)\n",
        "4. hidden_dim - tried values (256, 512)\n",
        "5. layer - tried values (2, 3, 4)\n",
        "\n",
        "Some best configs:\n",
        "1. learning rate (0.01), layer (2), epoch (5) - accuracy (0.8646)\n",
        "2. learning rate (0.001), layer (2), epoch (10) - accuracy (0.8625)\n",
        "3. learning rate (0.1), layer (2), epoch (10) - accuracy (0.8109)\n",
        "4. learning rate (0.01), layer (3), epoch (10) - accuracy (**0.8784**)\n",
        "\n",
        "I used different configurations of hyperparametes to get the best test accuracy. From the above some different configurations, we can see the test accuracy is the highest for learning rate 0.01 and layer 3."
      ],
      "metadata": {
        "id": "nrXJRYx-FTCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2"
      ],
      "metadata": {
        "id": "aRL4-aSHPCfo"
      }
    }
  ]
}